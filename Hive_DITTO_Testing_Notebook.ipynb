{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive-DITTO Integration Testing Notebook\n",
    "\n",
    "This notebook allows you to test the complete Hive data extraction, DITTO entity matching, and result storage workflow before deploying to Kubeflow.\n",
    "\n",
    "## Workflow Overview:\n",
    "1. **Setup & Configuration** - Configure connections and parameters\n",
    "2. **Data Extraction** - Extract data from Hive table to JSONL\n",
    "3. **Data Preprocessing** - Convert to DITTO format\n",
    "4. **Entity Matching** - Run DITTO matching\n",
    "5. **Result Analysis** - Analyze matching results\n",
    "6. **Save to Hive** - Store results back to Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pyhive pandas thrift sasl jsonlines matplotlib seaborn\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úì Setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hive Configuration\n",
    "HIVE_CONFIG = {\n",
    "    'host': 'localhost',  # Change to your Hive host\n",
    "    'port': 10000,\n",
    "    'database': 'default',\n",
    "    'username': 'hive',  # Change to your username\n",
    "    'auth': 'NOSASL'     # Options: 'NOSASL', 'PLAIN', 'KERBEROS'\n",
    "}\n",
    "\n",
    "# Input/Output Configuration\n",
    "DATA_CONFIG = {\n",
    "    'input_table': 'base.table',              # Your Hive input table\n",
    "    'output_table': 'results.ditto_matches',  # Hive output table\n",
    "    'temp_dir': './temp_notebook',            # Temporary directory\n",
    "    'input_jsonl': './temp_notebook/input_data.jsonl',\n",
    "    'output_jsonl': './temp_notebook/output_results.jsonl',\n",
    "    'sample_size': 1000                       # Number of records to test with\n",
    "}\n",
    "\n",
    "# DITTO Configuration\n",
    "DITTO_CONFIG = {\n",
    "    'task': 'person_records',\n",
    "    'lm': 'bert',\n",
    "    'max_len': 64,\n",
    "    'checkpoint_path': 'checkpoints/',\n",
    "    'use_gpu': True,\n",
    "    'fp16': True\n",
    "}\n",
    "\n",
    "# Create temp directory\n",
    "os.makedirs(DATA_CONFIG['temp_dir'], exist_ok=True)\n",
    "\n",
    "print(\"üìã Configuration loaded:\")\n",
    "print(f\"  Hive: {HIVE_CONFIG['host']}:{HIVE_CONFIG['port']}\")\n",
    "print(f\"  Input table: {DATA_CONFIG['input_table']}\")\n",
    "print(f\"  Output table: {DATA_CONFIG['output_table']}\")\n",
    "print(f\"  DITTO model: {DITTO_CONFIG['lm']} ({DITTO_CONFIG['task']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiveConnector:\n",
    "    \"\"\"Handle Hive database connections and operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.connection = None\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to Hive.\"\"\"\n",
    "        try:\n",
    "            from pyhive import hive\n",
    "            self.connection = hive.Connection(\n",
    "                host=self.config['host'],\n",
    "                port=self.config['port'],\n",
    "                database=self.config['database'],\n",
    "                username=self.config.get('username'),\n",
    "                auth=self.config.get('auth', 'NOSASL')\n",
    "            )\n",
    "            logger.info(f\"Connected to Hive: {self.config['host']}:{self.config['port']}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to connect to Hive: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def execute_query(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"Execute SQL query and return results as DataFrame.\"\"\"\n",
    "        if not self.connection:\n",
    "            raise RuntimeError(\"Not connected to Hive\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_sql(query, self.connection)\n",
    "            logger.info(f\"Query executed successfully, returned {len(df)} rows\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Query failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Hive connection.\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            logger.info(\"Hive connection closed\")\n",
    "\n",
    "def convert_to_ditto_format(df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Convert DataFrame to DITTO JSONL format.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Convert row to COL/VAL format\n",
    "        col_val_parts = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                value = str(row[col]).strip()\n",
    "                col_val_parts.append(f\"COL {col} VAL {value}\")\n",
    "        \n",
    "        record_text = \" \".join(col_val_parts)\n",
    "        \n",
    "        # Create JSONL record (modify this logic based on your needs)\n",
    "        record = {\n",
    "            \"left\": record_text,\n",
    "            \"right\": record_text,  # For self-matching, change as needed\n",
    "            \"id\": idx\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "def analyze_ditto_results(jsonl_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze DITTO matching results.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                results.append(json.loads(line))\n",
    "    \n",
    "    if not results:\n",
    "        return {\"error\": \"No results found\"}\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_pairs = len(results)\n",
    "    matches = sum(1 for r in results if r.get('match', False))\n",
    "    match_scores = [r.get('match_confidence', 0.0) for r in results]\n",
    "    \n",
    "    return {\n",
    "        'total_pairs': total_pairs,\n",
    "        'matches': matches,\n",
    "        'non_matches': total_pairs - matches,\n",
    "        'match_rate': matches / total_pairs if total_pairs > 0 else 0,\n",
    "        'avg_score': np.mean(match_scores) if match_scores else 0,\n",
    "        'score_distribution': match_scores\n",
    "    }\n",
    "\n",
    "print(\"‚úì Utility functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Hive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Hive connection\n",
    "hive_conn = HiveConnector(HIVE_CONFIG)\n",
    "\n",
    "if hive_conn.connect():\n",
    "    print(\"‚úÖ Hive connection successful!\")\n",
    "    \n",
    "    # Test with a simple query\n",
    "    try:\n",
    "        test_df = hive_conn.execute_query(\"SHOW TABLES LIMIT 5\")\n",
    "        print(\"\\nüìã Available tables (sample):\")\n",
    "        display(test_df)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not list tables: {e}\")\n",
    "        print(\"This might be normal depending on your Hive permissions\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Hive connection failed!\")\n",
    "    print(\"Please check your configuration and ensure Hive is accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Extraction from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from Hive table\n",
    "print(f\"üîÑ Extracting data from: {DATA_CONFIG['input_table']}\")\n",
    "\n",
    "try:\n",
    "    # Extract sample data\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM {DATA_CONFIG['input_table']} \n",
    "    LIMIT {DATA_CONFIG['sample_size']}\n",
    "    \"\"\"\n",
    "    \n",
    "    df = hive_conn.execute_query(query)\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(df)} records\")\n",
    "    print(f\"üìä Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nüìã Sample data:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Show data info\n",
    "    print(\"\\nüìà Data info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data extraction failed: {e}\")\n",
    "    print(\"\\nüí° Creating sample data for testing...\")\n",
    "    \n",
    "    # Create sample data if Hive extraction fails\n",
    "    df = pd.DataFrame({\n",
    "        'id': range(1, 11),\n",
    "        'firstname': ['Ahmed', 'Mohamed', 'Fatima', 'Omar', 'Aisha', 'Khalid', 'Layla', 'Hassan', 'Zeinab', 'Ali'],\n",
    "        'lastname': ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', 'Martinez'],\n",
    "        'email': [f'user{i}@example.com' for i in range(1, 11)],\n",
    "        'phone': [f'555-000{i:04d}' for i in range(1, 11)]\n",
    "    })\n",
    "    print(f\"‚úÖ Created sample dataset with {len(df)} records\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convert to DITTO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DITTO format\n",
    "print(\"üîÑ Converting data to DITTO format...\")\n",
    "\n",
    "ditto_records = convert_to_ditto_format(df)\n",
    "\n",
    "print(f\"‚úÖ Converted {len(ditto_records)} records to DITTO format\")\n",
    "\n",
    "# Save to JSONL file\n",
    "with open(DATA_CONFIG['input_jsonl'], 'w', encoding='utf-8') as f:\n",
    "    for record in ditto_records:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"üíæ Saved to: {DATA_CONFIG['input_jsonl']}\")\n",
    "\n",
    "# Show sample records\n",
    "print(\"\\nüìã Sample DITTO records:\")\n",
    "for i, record in enumerate(ditto_records[:3]):\n",
    "    print(f\"\\nRecord {i+1}:\")\n",
    "    print(f\"  Left:  {record['left'][:100]}...\")\n",
    "    print(f\"  Right: {record['right'][:100]}...\")\n",
    "    print(f\"  ID:    {record['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check DITTO Model Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if DITTO models and checkpoints are available\n",
    "print(\"üîç Checking DITTO model status...\")\n",
    "\n",
    "# Check model files\n",
    "model_checks = {\n",
    "    'BERT Model': './models/bert-base-uncased/config.json',\n",
    "    'RoBERTa Model': './models/roberta-base/config.json',\n",
    "    'DistilBERT Model': './models/distilbert-base-uncased/config.json',\n",
    "    'DITTO Matcher': './matcher.py',\n",
    "    'Checkpoint Directory': DITTO_CONFIG['checkpoint_path']\n",
    "}\n",
    "\n",
    "for name, path in model_checks.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ {name}: Found at {path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: Not found at {path}\")\n",
    "\n",
    "# Set environment variables for offline models\n",
    "os.environ['BERT_MODEL_PATH'] = './models/bert-base-uncased'\n",
    "os.environ['ROBERTA_MODEL_PATH'] = './models/roberta-base'\n",
    "os.environ['DISTILBERT_MODEL_PATH'] = './models/distilbert-base-uncased'\n",
    "os.environ['NLTK_DATA'] = './nltk_data'\n",
    "\n",
    "print(\"\\nüîß Environment variables set for offline model usage\")\n",
    "\n",
    "# Check GPU availability\n",
    "gpu_available = False\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        gpu_available = True\n",
    "        print(\"üéÆ GPU available for DITTO matching\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  GPU not available, will use CPU\")\n",
    "        DITTO_CONFIG['use_gpu'] = False\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Cannot detect GPU, will use CPU\")\n",
    "    DITTO_CONFIG['use_gpu'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run DITTO Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run DITTO matching\n",
    "print(\"üîÑ Running DITTO entity matching...\")\n",
    "\n",
    "# Build matcher command\n",
    "cmd = [\n",
    "    'python', 'matcher.py',\n",
    "    '--task', DITTO_CONFIG['task'],\n",
    "    '--input_path', DATA_CONFIG['input_jsonl'],\n",
    "    '--output_path', DATA_CONFIG['output_jsonl'],\n",
    "    '--lm', DITTO_CONFIG['lm'],\n",
    "    '--max_len', str(DITTO_CONFIG['max_len']),\n",
    "    '--checkpoint_path', DITTO_CONFIG['checkpoint_path']\n",
    "]\n",
    "\n",
    "if DITTO_CONFIG['use_gpu']:\n",
    "    cmd.append('--use_gpu')\n",
    "    \n",
    "if DITTO_CONFIG['fp16']:\n",
    "    cmd.append('--fp16')\n",
    "\n",
    "print(f\"üöÄ Command: {' '.join(cmd)}\")\n",
    "\n",
    "# Set environment for GPU\n",
    "env = os.environ.copy()\n",
    "if DITTO_CONFIG['use_gpu']:\n",
    "    env['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "try:\n",
    "    # Run the matcher\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        env=env,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=300  # 5 minute timeout\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ DITTO matching completed successfully!\")\n",
    "        print(\"\\nüìã Output:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ùå DITTO matching failed!\")\n",
    "        print(\"\\n‚ùå Error:\")\n",
    "        print(result.stderr)\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è∞ DITTO matching timed out\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error running DITTO: {e}\")\n",
    "\n",
    "# Check if output file exists\n",
    "if os.path.exists(DATA_CONFIG['output_jsonl']):\n",
    "    file_size = os.path.getsize(DATA_CONFIG['output_jsonl'])\n",
    "    print(f\"\\nüíæ Output file created: {DATA_CONFIG['output_jsonl']} ({file_size} bytes)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Output file not created: {DATA_CONFIG['output_jsonl']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze DITTO results\n",
    "if os.path.exists(DATA_CONFIG['output_jsonl']):\n",
    "    print(\"üìä Analyzing DITTO matching results...\")\n",
    "    \n",
    "    stats = analyze_ditto_results(DATA_CONFIG['output_jsonl'])\n",
    "    \n",
    "    if 'error' not in stats:\n",
    "        print(\"\\n‚úÖ Results Summary:\")\n",
    "        print(f\"  Total pairs processed: {stats['total_pairs']}\")\n",
    "        print(f\"  Matches found: {stats['matches']}\")\n",
    "        print(f\"  Non-matches: {stats['non_matches']}\")\n",
    "        print(f\"  Match rate: {stats['match_rate']:.2%}\")\n",
    "        print(f\"  Average confidence: {stats['avg_score']:.3f}\")\n",
    "        \n",
    "        # Plot score distribution\n",
    "        if stats['score_distribution']:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(stats['score_distribution'], bins=20, alpha=0.7, edgecolor='black')\n",
    "            plt.xlabel('Match Confidence Score')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title('Distribution of Match Confidence Scores')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            match_counts = [stats['matches'], stats['non_matches']]\n",
    "            labels = ['Matches', 'Non-matches']\n",
    "            plt.pie(match_counts, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "            plt.title('Match vs Non-match Distribution')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Show sample results\n",
    "        print(\"\\nüìã Sample Results:\")\n",
    "        with open(DATA_CONFIG['output_jsonl'], 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 5:  # Show first 5 results\n",
    "                    break\n",
    "                result = json.loads(line)\n",
    "                match_status = \"‚úÖ MATCH\" if result.get('match', False) else \"‚ùå NO MATCH\"\n",
    "                confidence = result.get('match_confidence', 0.0)\n",
    "                print(f\"\\nResult {i+1}: {match_status} (confidence: {confidence:.3f})\")\n",
    "                print(f\"  Left:  {result.get('left', '')[:80]}...\")\n",
    "                print(f\"  Right: {result.get('right', '')[:80]}...\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"‚ùå Error analyzing results: {stats['error']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No results file found to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results back to Hive\n",
    "if os.path.exists(DATA_CONFIG['output_jsonl']):\n",
    "    print(f\"üîÑ Saving results to Hive table: {DATA_CONFIG['output_table']}\")\n",
    "    \n",
    "    try:\n",
    "        # Read results\n",
    "        results = []\n",
    "        with open(DATA_CONFIG['output_jsonl'], 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    results.append(json.loads(line))\n",
    "        \n",
    "        print(f\"üìã Loaded {len(results)} results\")\n",
    "        \n",
    "        # Convert to DataFrame for easier handling\n",
    "        results_df = pd.DataFrame([\n",
    "            {\n",
    "                'record_id': r.get('id', 0),\n",
    "                'left_record': r.get('left', ''),\n",
    "                'right_record': r.get('right', ''),\n",
    "                'match_probability': r.get('match_confidence', 0.0),\n",
    "                'is_match': r.get('match', False),\n",
    "                'created_at': datetime.now().isoformat()\n",
    "            }\n",
    "            for r in results\n",
    "        ])\n",
    "        \n",
    "        print(\"\\nüìä Results DataFrame:\")\n",
    "        display(results_df.head())\n",
    "        \n",
    "        # Connect to Hive for saving\n",
    "        if hive_conn.connection:\n",
    "            cursor = hive_conn.connection.cursor()\n",
    "            \n",
    "            # Create table if doesn't exist\n",
    "            create_table_sql = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {DATA_CONFIG['output_table']} (\n",
    "                record_id INT,\n",
    "                left_record STRING,\n",
    "                right_record STRING,\n",
    "                match_probability DOUBLE,\n",
    "                is_match BOOLEAN,\n",
    "                created_at STRING\n",
    "            )\n",
    "            STORED AS PARQUET\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(create_table_sql)\n",
    "            print(f\"‚úÖ Table {DATA_CONFIG['output_table']} created/verified\")\n",
    "            \n",
    "            # Insert results (for large datasets, consider bulk loading)\n",
    "            insert_count = 0\n",
    "            for _, row in results_df.iterrows():\n",
    "                # Escape single quotes in strings\n",
    "                left_record = str(row['left_record']).replace(\"'\", \"''\")[:1000]  # Truncate long records\n",
    "                right_record = str(row['right_record']).replace(\"'\", \"''\")[:1000]\n",
    "                \n",
    "                insert_sql = f\"\"\"\n",
    "                INSERT INTO {DATA_CONFIG['output_table']} VALUES (\n",
    "                    {row['record_id']},\n",
    "                    '{left_record}',\n",
    "                    '{right_record}',\n",
    "                    {row['match_probability']},\n",
    "                    {'true' if row['is_match'] else 'false'},\n",
    "                    '{row['created_at']}'\n",
    "                )\n",
    "                \"\"\"\n",
    "                \n",
    "                cursor.execute(insert_sql)\n",
    "                insert_count += 1\n",
    "                \n",
    "                if insert_count % 10 == 0:\n",
    "                    print(f\"  Inserted {insert_count}/{len(results_df)} records...\")\n",
    "            \n",
    "            print(f\"‚úÖ Successfully saved {insert_count} results to {DATA_CONFIG['output_table']}\")\n",
    "            \n",
    "            # Verify insertion\n",
    "            verify_df = hive_conn.execute_query(f\"SELECT COUNT(*) as count FROM {DATA_CONFIG['output_table']}\")\n",
    "            print(f\"üîç Verification: {verify_df.iloc[0]['count']} records in table\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå Not connected to Hive, cannot save results\")\n",
    "            print(\"üí° Results are available in the DataFrame above and the JSONL file\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving to Hive: {e}\")\n",
    "        print(\"üí° Results are still available in the JSONL file for manual processing\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No results file found to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and summary\n",
    "print(\"üßπ Cleaning up and generating summary...\")\n",
    "\n",
    "# Close Hive connection\n",
    "hive_conn.close()\n",
    "\n",
    "# Generate workflow summary\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'configuration': {\n",
    "        'hive_table': DATA_CONFIG['input_table'],\n",
    "        'output_table': DATA_CONFIG['output_table'],\n",
    "        'ditto_model': DITTO_CONFIG['lm'],\n",
    "        'sample_size': DATA_CONFIG['sample_size']\n",
    "    },\n",
    "    'files_created': [\n",
    "        DATA_CONFIG['input_jsonl'],\n",
    "        DATA_CONFIG['output_jsonl']\n",
    "    ],\n",
    "    'status': 'completed'\n",
    "}\n",
    "\n",
    "# Add results statistics if available\n",
    "if os.path.exists(DATA_CONFIG['output_jsonl']):\n",
    "    stats = analyze_ditto_results(DATA_CONFIG['output_jsonl'])\n",
    "    if 'error' not in stats:\n",
    "        summary['results'] = stats\n",
    "\n",
    "# Save summary\n",
    "summary_path = os.path.join(DATA_CONFIG['temp_dir'], 'workflow_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"üìã Summary saved to: {summary_path}\")\n",
    "\n",
    "print(\"\\nüéâ Workflow Testing Complete!\")\n",
    "print(\"\\nüìã Next Steps for Kubeflow Deployment:\")\n",
    "print(\"1. ‚úÖ Test completed successfully - ready for Kubeflow packaging\")\n",
    "print(\"2. üîß Update configuration parameters in the Kubeflow pipeline\")\n",
    "print(\"3. üê≥ Build Docker image with all dependencies\")\n",
    "print(\"4. üöÄ Deploy pipeline to Kubeflow\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "for file_path in [DATA_CONFIG['input_jsonl'], DATA_CONFIG['output_jsonl'], summary_path]:\n",
    "    if os.path.exists(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"  ‚úÖ {file_path} ({size} bytes)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {file_path} (not created)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Kubeflow Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate configuration for Kubeflow pipeline based on test results\n",
    "print(\"üîß Generating Kubeflow pipeline configuration...\")\n",
    "\n",
    "pipeline_config = {\n",
    "    \"pipeline_name\": \"hive-ditto-entity-matching\",\n",
    "    \"description\": \"Entity matching pipeline with Hive integration\",\n",
    "    \"parameters\": {\n",
    "        \"hive_host\": HIVE_CONFIG['host'],\n",
    "        \"hive_port\": HIVE_CONFIG['port'],\n",
    "        \"hive_database\": HIVE_CONFIG['database'],\n",
    "        \"hive_username\": HIVE_CONFIG['username'],\n",
    "        \"input_table\": DATA_CONFIG['input_table'],\n",
    "        \"output_table\": DATA_CONFIG['output_table'],\n",
    "        \"ditto_task\": DITTO_CONFIG['task'],\n",
    "        \"ditto_lm\": DITTO_CONFIG['lm'],\n",
    "        \"ditto_max_len\": DITTO_CONFIG['max_len'],\n",
    "        \"checkpoint_path\": DITTO_CONFIG['checkpoint_path'],\n",
    "        \"use_gpu\": DITTO_CONFIG['use_gpu'],\n",
    "        \"fp16\": DITTO_CONFIG['fp16']\n",
    "    },\n",
    "    \"resources\": {\n",
    "        \"extract_step\": {\n",
    "            \"cpu\": \"2\",\n",
    "            \"memory\": \"4Gi\"\n",
    "        },\n",
    "        \"matching_step\": {\n",
    "            \"cpu\": \"4\",\n",
    "            \"memory\": \"8Gi\",\n",
    "            \"gpu\": \"1\" if DITTO_CONFIG['use_gpu'] else \"0\"\n",
    "        },\n",
    "        \"save_step\": {\n",
    "            \"cpu\": \"2\",\n",
    "            \"memory\": \"4Gi\"\n",
    "        }\n",
    "    },\n",
    "    \"volumes\": {\n",
    "        \"data_volume\": \"50Gi\",\n",
    "        \"models_volume\": \"10Gi\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save pipeline configuration\n",
    "config_path = os.path.join(DATA_CONFIG['temp_dir'], 'kubeflow_pipeline_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(pipeline_config, f, indent=2)\n",
    "\n",
    "print(f\"üìã Pipeline configuration saved to: {config_path}\")\n",
    "\n",
    "# Generate Docker build commands\n",
    "docker_commands = f\"\"\"\n",
    "# Docker Build Commands for Kubeflow Deployment\n",
    "\n",
    "# 1. Build the DITTO image with Hive support\n",
    "docker build -t your-registry/ditto-hive:latest -f Dockerfile .\n",
    "\n",
    "# 2. Push to your container registry\n",
    "docker push your-registry/ditto-hive:latest\n",
    "\n",
    "# 3. Update the pipeline image references\n",
    "# Edit hive_ditto_pipeline.py and update base_image='your-registry/ditto-hive:latest'\n",
    "\n",
    "# 4. Compile the pipeline\n",
    "python hive_ditto_pipeline.py\n",
    "\n",
    "# 5. Upload the compiled pipeline YAML to Kubeflow\n",
    "\"\"\"\n",
    "\n",
    "docker_script_path = os.path.join(DATA_CONFIG['temp_dir'], 'docker_build_commands.txt')\n",
    "with open(docker_script_path, 'w') as f:\n",
    "    f.write(docker_commands)\n",
    "\n",
    "print(f\"üê≥ Docker build commands saved to: {docker_script_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Kubeflow deployment preparation complete!\")\n",
    "print(\"\\nüìã Configuration Summary:\")\n",
    "print(json.dumps(pipeline_config, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}